[
  {
    "objectID": "distributions.html",
    "href": "distributions.html",
    "title": "Distributions:",
    "section": "",
    "text": "Distributions are ways to describe how a phenomena behaves. I.e. how does flipping a coin behave? Rolling a die? How does height in a population behave? How does radioactive waste decay? Although phenomena can seems unrelated a lot of them behave similarly. The result of a coinflip follows the same kind distribution as the result of any other binary outcome. A list and representation of some of the most common distributions can be seen underneath. In the github page for this section you can find a bunch of Shiny-apps where you can interactively change parameters of the distributions so download that if you want. Another concept is central to the understanding of distributions. Support. Finite support is when a distribution has a finite number of outcomes. Infinite support distributions is when there is an infinite amount of outcomes.\n\n\nDiscrete distributions are associated with discrete data. Data that falls within distinct categories. The most basic discrete probability distribution is the bernoulli distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Bernoulli distribution is so simple we often don’t even consider it a distribution. The bernoulli distribution describes two outcomes. I.e. Flipping a coin. This is done one time. It does so with a particular a particular probability. It is probably the distribution that makes the most intuitive sense, without actually making sense as a distriribution. I think this is because distributions are usually considered to describe an array of outputs, not just two values. This can be illustrated by the following app. for the Bernoulli distribution the only parameter that changes is p. The probability density function is:\n\n\n\\[f(k;p) = Prob(X = k) =  p^k(1-p)^{1-k}  \\: \\: \\: \\: for \\: k \\: in \\: \\{0,1\\}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nThe binomial distribution is based on the bernoulli distribution. More specifically it describes the probability of getting a specific number of successes out of N independent bernoulli trials. The formula the density function of a binomial distribution is:\n\\[f(k, n, p) = Prob(X = k) = \\binom{n}{k}  p^k(1-p)^{1-k}\\]\nWhere p is the probability of every trial being a succes. k is the number of successes. n is the number of bernoulli trials. \\(\\binom{n}{k}\\) is the binomial coefficient. Basicallye the binomial coefficient calculates how many different ways we can choose k elements from n elements. E.g. \\(\\binom{4}{2} = 6\\) because there are six different ways to choose a subset of two from a set of four different values. The Binomial coefficient can be calculated by:\n\\[\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\]\n\n\n\nSo the hypergeometric distribution is very much like the binomial distribution. The difference is that there is no replacement in the hypergeometric distribution. This means that every trial has a different probability of success. The probability density function is given by:\n\\[Prob(X = k) = \\frac{\\binom{K}{k} \\binom{N-K}{n-k}}{\\binom{N}{n}}\\]\nWhere N is the total population. n is the number of trials. K is the total amount of Successes in the initial population. k is a a number of successes\n\\(\\binom{K}{k}\\) describes how many ways we can select k successes from K total successes. \\(\\binom{N-K}{n-k}\\) describes how many ways we the remaining failures can be arranged. \\(\\binom{N}{n}\\) Describes in how many different ways we can choose n samples from N total samples.\nIn other words: If there are a lot of ways to choose the amount of successes from total successes, and the amount of failures from total failures in n trials, there is a large probability.\nThe probability density function is:\n\n\n\nThe Poisson Binomial distribution is somewhat similar to the Beta binomial distribution. But instead of sampling the probability from a distribution, we know the probability of each outcome. In the example below, I have sampled, since it did not seem feasible to put in a lot of them by hand. The probability can be written as a sum:\n\\[Pr(K = k) = \\sum_{A \\in F_k} \\prod_{i \\in A} p_i \\prod_{j \\in A^c} (1-p_j)\\]\n\\(F_k\\) is all the combinations of k integers that can be selected from 1:n. example: \\(F_2\\) , n = 1,2,3 = {{1,2}, {1,3}, {2,3}}. A is the set of a particular set combination, e.g. {1,2} in the above example. \\(A^c\\) is the complementary of A, e.g. A = {{1,2}}, Ac = {{1,3},{2,3}}.\nGenerally speaking this means that the probabilty of a particular k (number of successes) is the sum of the product of probability of combinations.\n\n\n\nThe beta binomial distribution is another layer of complexity. Like the Binomial distribution it assumes that trials are independent bernoulli distributed. However, it does not assume that the probability is the same for every trial, but that the probabilty of every trial is sampled from a beta distribution (see section on continuous distributions). That is:\nIf p ~ Beta(\\(\\alpha\\), \\(\\beta\\)) and X ~ Binomial(n, p), then X ~ BetaBinomial(n, \\(\\alpha\\), \\(\\beta\\)).\nAnd the pdf can be written as:\n\\[f(x | n, \\alpha, \\beta) = \\int_0^1 Bin(x | n, p) \\: Beta(p | \\alpha, \\beta ) \\: dp \\:=  \\binom{N}{x} \\frac{B(x + \\alpha, n - x - \\beta)}{B(\\alpha, \\beta)}\\]\n\n\n\nInstead of modelling the probability of successes in a certain amount of pulls, the negative binomial distribution models the how many failures we can expect before we get a certain number of successes. The probability distribution is given by:\n\\[X \\sim NB(r, p)\\]\n\\[f(k; r, p) = Pr(X = k) = \\binom{k + r - 1}{k}(1-p)^kp^r\\]\nWhere k is the number of failures. r is the number of successes. The binomial coefficient describes in how many different ways k failures can be chosen between k failures and r successes (the minus one is because the last pull is always a success).\n\n\n\nConceptually A bit like the negative Binomial distribution. Instead of finding the probabilities of how many successes are drawn, we find the probabilities of how many balls are drawn before a certain number of successes are drawn. The Probability density function cna be written as:\n\\[Pr(X = k) = \\frac{\\binom{k+r-1}{k} \\binom{N-r-k}{K-k}}{\\binom{N}{K}}\\]\nWhere N is the population size. K is the total amount of successes. r is the number of failures. k is the number of observed successes.\n\n\n\n\n\n\nThe poisson distribution is used a lot in different biological and physical scenarios. It was first developed to describe how likely it is to see a certain amount of events within a certain time frame. E.g. how likely is it to see 1, 2, 3 (etc) in cars outside your window in an hour. The probability density function is:\n\\[Pr(X = k) = \\frac{\\lambda^ke^{-k}}{k!}\\]\n\n\n\nThe beta negative binomial distribution describes the probability of getting k failures before getting r successes. However, the probability of each success is distributed with a beta distribution. The pdf is given as:\n\\[X \\sim BNB(r, \\alpha, \\beta) = \\int_0^1f_{X | p }(k|r,q) \\cdot f_p(q | \\alpha, \\beta)dq = \\frac{B(r+k, \\alpha + \\beta)}{B(r, \\alpha)} \\frac{\\Gamma(k + \\beta)}{k!\\Gamma(\\beta)}\\]\nThis involves some math. However, it can be explained in an easier way. F(X|p) describes the conditional probability of drawing k failures before n\n\n\n\nThe geometric distribution describes how many bernoulli trials it takes to get a single success. The pdf is given by:\n\\[Pr(X = k) = (1-p)^{k-1}p\\]"
  },
  {
    "objectID": "distributions.html#discrete-distribution",
    "href": "distributions.html#discrete-distribution",
    "title": "Distributions:",
    "section": "",
    "text": "Discrete distributions are associated with discrete data. Data that falls within distinct categories. The most basic discrete probability distribution is the bernoulli distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Bernoulli distribution is so simple we often don’t even consider it a distribution. The bernoulli distribution describes two outcomes. I.e. Flipping a coin. This is done one time. It does so with a particular a particular probability. It is probably the distribution that makes the most intuitive sense, without actually making sense as a distriribution. I think this is because distributions are usually considered to describe an array of outputs, not just two values. This can be illustrated by the following app. for the Bernoulli distribution the only parameter that changes is p. The probability density function is:\n\n\n\\[f(k;p) = Prob(X = k) =  p^k(1-p)^{1-k}  \\: \\: \\: \\: for \\: k \\: in \\: \\{0,1\\}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nThe binomial distribution is based on the bernoulli distribution. More specifically it describes the probability of getting a specific number of successes out of N independent bernoulli trials. The formula the density function of a binomial distribution is:\n\\[f(k, n, p) = Prob(X = k) = \\binom{n}{k}  p^k(1-p)^{1-k}\\]\nWhere p is the probability of every trial being a succes. k is the number of successes. n is the number of bernoulli trials. \\(\\binom{n}{k}\\) is the binomial coefficient. Basicallye the binomial coefficient calculates how many different ways we can choose k elements from n elements. E.g. \\(\\binom{4}{2} = 6\\) because there are six different ways to choose a subset of two from a set of four different values. The Binomial coefficient can be calculated by:\n\\[\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\]\n\n\n\nSo the hypergeometric distribution is very much like the binomial distribution. The difference is that there is no replacement in the hypergeometric distribution. This means that every trial has a different probability of success. The probability density function is given by:\n\\[Prob(X = k) = \\frac{\\binom{K}{k} \\binom{N-K}{n-k}}{\\binom{N}{n}}\\]\nWhere N is the total population. n is the number of trials. K is the total amount of Successes in the initial population. k is a a number of successes\n\\(\\binom{K}{k}\\) describes how many ways we can select k successes from K total successes. \\(\\binom{N-K}{n-k}\\) describes how many ways we the remaining failures can be arranged. \\(\\binom{N}{n}\\) Describes in how many different ways we can choose n samples from N total samples.\nIn other words: If there are a lot of ways to choose the amount of successes from total successes, and the amount of failures from total failures in n trials, there is a large probability.\nThe probability density function is:\n\n\n\nThe Poisson Binomial distribution is somewhat similar to the Beta binomial distribution. But instead of sampling the probability from a distribution, we know the probability of each outcome. In the example below, I have sampled, since it did not seem feasible to put in a lot of them by hand. The probability can be written as a sum:\n\\[Pr(K = k) = \\sum_{A \\in F_k} \\prod_{i \\in A} p_i \\prod_{j \\in A^c} (1-p_j)\\]\n\\(F_k\\) is all the combinations of k integers that can be selected from 1:n. example: \\(F_2\\) , n = 1,2,3 = {{1,2}, {1,3}, {2,3}}. A is the set of a particular set combination, e.g. {1,2} in the above example. \\(A^c\\) is the complementary of A, e.g. A = {{1,2}}, Ac = {{1,3},{2,3}}.\nGenerally speaking this means that the probabilty of a particular k (number of successes) is the sum of the product of probability of combinations.\n\n\n\nThe beta binomial distribution is another layer of complexity. Like the Binomial distribution it assumes that trials are independent bernoulli distributed. However, it does not assume that the probability is the same for every trial, but that the probabilty of every trial is sampled from a beta distribution (see section on continuous distributions). That is:\nIf p ~ Beta(\\(\\alpha\\), \\(\\beta\\)) and X ~ Binomial(n, p), then X ~ BetaBinomial(n, \\(\\alpha\\), \\(\\beta\\)).\nAnd the pdf can be written as:\n\\[f(x | n, \\alpha, \\beta) = \\int_0^1 Bin(x | n, p) \\: Beta(p | \\alpha, \\beta ) \\: dp \\:=  \\binom{N}{x} \\frac{B(x + \\alpha, n - x - \\beta)}{B(\\alpha, \\beta)}\\]\n\n\n\nInstead of modelling the probability of successes in a certain amount of pulls, the negative binomial distribution models the how many failures we can expect before we get a certain number of successes. The probability distribution is given by:\n\\[X \\sim NB(r, p)\\]\n\\[f(k; r, p) = Pr(X = k) = \\binom{k + r - 1}{k}(1-p)^kp^r\\]\nWhere k is the number of failures. r is the number of successes. The binomial coefficient describes in how many different ways k failures can be chosen between k failures and r successes (the minus one is because the last pull is always a success).\n\n\n\nConceptually A bit like the negative Binomial distribution. Instead of finding the probabilities of how many successes are drawn, we find the probabilities of how many balls are drawn before a certain number of successes are drawn. The Probability density function cna be written as:\n\\[Pr(X = k) = \\frac{\\binom{k+r-1}{k} \\binom{N-r-k}{K-k}}{\\binom{N}{K}}\\]\nWhere N is the population size. K is the total amount of successes. r is the number of failures. k is the number of observed successes.\n\n\n\n\n\n\nThe poisson distribution is used a lot in different biological and physical scenarios. It was first developed to describe how likely it is to see a certain amount of events within a certain time frame. E.g. how likely is it to see 1, 2, 3 (etc) in cars outside your window in an hour. The probability density function is:\n\\[Pr(X = k) = \\frac{\\lambda^ke^{-k}}{k!}\\]\n\n\n\nThe beta negative binomial distribution describes the probability of getting k failures before getting r successes. However, the probability of each success is distributed with a beta distribution. The pdf is given as:\n\\[X \\sim BNB(r, \\alpha, \\beta) = \\int_0^1f_{X | p }(k|r,q) \\cdot f_p(q | \\alpha, \\beta)dq = \\frac{B(r+k, \\alpha + \\beta)}{B(r, \\alpha)} \\frac{\\Gamma(k + \\beta)}{k!\\Gamma(\\beta)}\\]\nThis involves some math. However, it can be explained in an easier way. F(X|p) describes the conditional probability of drawing k failures before n\n\n\n\nThe geometric distribution describes how many bernoulli trials it takes to get a single success. The pdf is given by:\n\\[Pr(X = k) = (1-p)^{k-1}p\\]"
  },
  {
    "objectID": "quarto_venv/lib/python3.10/site-packages/soupsieve-2.7.dist-info/licenses/LICENSE.html",
    "href": "quarto_venv/lib/python3.10/site-packages/soupsieve-2.7.dist-info/licenses/LICENSE.html",
    "title": "Statistics Notes",
    "section": "",
    "text": "MIT License\nCopyright (c) 2018 - 2025 Isaac Muse isaacmuse@gmail.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "quarto_venv/lib/python3.10/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "href": "quarto_venv/lib/python3.10/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "title": "Statistics Notes",
    "section": "",
    "text": "Copyright © 2019, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "stats_intro.html",
    "href": "stats_intro.html",
    "title": "Statistics Notes",
    "section": "",
    "text": "So what is statistics even? The answer might differ depending on who you ask. I am a biologist - and my understanding of statistics reflects this. I acknowledge that the theory presented in this little presentation is simplified compared to what an actual statistician would have given, but if the goal is to use statistics, this might not be all bad. I will try my best to give a different take on how to understand statistics, than what I have seen in the statistics courses that I have taken. In as brief a way as possible.\n\n\nThe name statistics originate (according to Wikipedia) from states measuring different economic patterns. It uses the tools of mathematics to describe “real world” scenarios. Two overall parts of statistics are of particular importance: probability and statistical inference. The two are closely connected.\n\n\nProbability theory describes the likeliness of an outcome. In more formal terms probability theory wishes to attribute probability to outcomes expressed in a sample space. A sample space is the set of possible outcomes, i.e. heads and tails for flipping a coin. Statistical inference uses mathematical tools to estimate the properties of distribution of probability. A distribution of probability is how probabilites are assigned to outcomes. I can seem counter intuitive at first. Some examples make sense, i.e. the probability of a coin.\n\n\n\n\n\nThe probabilty of a fair coin is 0.5 i.e. equal likely outcomes\n\n\nOther times it is less clear that we are working with probabilities. A good example is height. On the right the height of 10000 people is illustrated as a histogram. We intuitively know that is more common to be “normal” height. I.e. the probability of this is higher. However, the concept is a little more abstract than in the case of the coin.\n\n\n\n\n\n\n\n\n\nDifferent natural phenomenons can be described using different statistical distributions. A lot of work has gone into finding different probability distributions.\n\n\n\nA lot of work has gone into finding different probability distributions. We need probabilty distributions to attempt to explain the world around us. It has turned out that a lot of things we measure (variables) seem to behave in the same ways - i.e. they follow similar distributions. The distributions are intrinsically linked to the type of data it describes. Data fall into two overall categories: Discrete and Continuous. An example of discrete data is our coin example above. Something that falls within definite categories. Often times with no numerical relationship with each other - tails does not have a numerical distance to heads. Continuous distributions are numbers such as height that exist on a number line.\nCommon for all distributions is that they can be mathematically expressed. This means each one comes with a number of assumptions. As such, this is not a problem. The problem arise, when we as researchers use a particular distribution to describe an outcome where the assumptions are violated. When the assumptions are violated, the underlying statistical inference we draw regarding the underlying natural phenomenon will be wrong. For a review of some of the common distributions see the distributions tab above.\n\n\n\nStatistical modelling uses some of the concepts described above to model real world phenomena. I will get further into how to do statistical modelling in the section of the name above. There are two main schools of thought when it comes to statistical modelling: frequentist and bayesian. See them under their corresponding tabs.\n\n\n\nMachine learning uses central statistical concepts to teach a machine patterns in data. I will cover some use-cases and speak to why it is statistics in the first place, but not everything will be covered.\n\n\n\nA branch of Machine Learning where the computer architecture resemebles neurons in the brain. More of an explanation in the corresponding tab."
  },
  {
    "objectID": "stats_intro.html#a-few-words-on-statistics-in-general.",
    "href": "stats_intro.html#a-few-words-on-statistics-in-general.",
    "title": "Statistics Notes",
    "section": "",
    "text": "So what is statistics even? The answer might differ depending on who you ask. I am a biologist - and my understanding of statistics reflects this. I acknowledge that the theory presented in this little presentation is simplified compared to what an actual statistician would have given, but if the goal is to use statistics, this might not be all bad. I will try my best to give a different take on how to understand statistics, than what I have seen in the statistics courses that I have taken. In as brief a way as possible.\n\n\nThe name statistics originate (according to Wikipedia) from states measuring different economic patterns. It uses the tools of mathematics to describe “real world” scenarios. Two overall parts of statistics are of particular importance: probability and statistical inference. The two are closely connected.\n\n\nProbability theory describes the likeliness of an outcome. In more formal terms probability theory wishes to attribute probability to outcomes expressed in a sample space. A sample space is the set of possible outcomes, i.e. heads and tails for flipping a coin. Statistical inference uses mathematical tools to estimate the properties of distribution of probability. A distribution of probability is how probabilites are assigned to outcomes. I can seem counter intuitive at first. Some examples make sense, i.e. the probability of a coin.\n\n\n\n\n\nThe probabilty of a fair coin is 0.5 i.e. equal likely outcomes\n\n\nOther times it is less clear that we are working with probabilities. A good example is height. On the right the height of 10000 people is illustrated as a histogram. We intuitively know that is more common to be “normal” height. I.e. the probability of this is higher. However, the concept is a little more abstract than in the case of the coin.\n\n\n\n\n\n\n\n\n\nDifferent natural phenomenons can be described using different statistical distributions. A lot of work has gone into finding different probability distributions.\n\n\n\nA lot of work has gone into finding different probability distributions. We need probabilty distributions to attempt to explain the world around us. It has turned out that a lot of things we measure (variables) seem to behave in the same ways - i.e. they follow similar distributions. The distributions are intrinsically linked to the type of data it describes. Data fall into two overall categories: Discrete and Continuous. An example of discrete data is our coin example above. Something that falls within definite categories. Often times with no numerical relationship with each other - tails does not have a numerical distance to heads. Continuous distributions are numbers such as height that exist on a number line.\nCommon for all distributions is that they can be mathematically expressed. This means each one comes with a number of assumptions. As such, this is not a problem. The problem arise, when we as researchers use a particular distribution to describe an outcome where the assumptions are violated. When the assumptions are violated, the underlying statistical inference we draw regarding the underlying natural phenomenon will be wrong. For a review of some of the common distributions see the distributions tab above.\n\n\n\nStatistical modelling uses some of the concepts described above to model real world phenomena. I will get further into how to do statistical modelling in the section of the name above. There are two main schools of thought when it comes to statistical modelling: frequentist and bayesian. See them under their corresponding tabs.\n\n\n\nMachine learning uses central statistical concepts to teach a machine patterns in data. I will cover some use-cases and speak to why it is statistics in the first place, but not everything will be covered.\n\n\n\nA branch of Machine Learning where the computer architecture resemebles neurons in the brain. More of an explanation in the corresponding tab."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello and welcome",
    "section": "",
    "text": "This document is meant as a review / for improving my own understanding of statistics. Use it if you can but I take no responsibility for what is right and wrong. It is meant to be very hands on with code. Most often only the output is shown, but if you want to see it it is all available on my github page.\n\n\nThe page is made up of a mixture between descriptive text, figures and some interactive elements. Both Python and R code is used. Some of the output is placed in the right margine like so (for R and python respectively):\n\n\n\n\n\n\n[1] \"/usr/lib/R/bin/R\"\n\n\n\n\n\n/home/jenswaaben/phd/courses/stats_personal/stats_quarto/quarto_venv/bin/python\n\n\n\n \nSome code is embedded in the main-panel not the margin (R and python respectively).\n\n1+1 \n\n[1] 2\n\n\n\n1+1 \n\n2\n\n\n\n\n\nand figures are also included either in the main text:\n\nOr in the margin:\n\n\n\n\n\n\nThe sub-topics of statistics can be seen in the navigation bar at the top. For me the logical way to approach it is from left to right, but feel free to do it however you like."
  },
  {
    "objectID": "index.html#how-to-read-the-page.",
    "href": "index.html#how-to-read-the-page.",
    "title": "Hello and welcome",
    "section": "",
    "text": "The page is made up of a mixture between descriptive text, figures and some interactive elements. Both Python and R code is used. Some of the output is placed in the right margine like so (for R and python respectively):\n\n\n\n\n\n\n[1] \"/usr/lib/R/bin/R\"\n\n\n\n\n\n/home/jenswaaben/phd/courses/stats_personal/stats_quarto/quarto_venv/bin/python\n\n\n\n \nSome code is embedded in the main-panel not the margin (R and python respectively).\n\n1+1 \n\n[1] 2\n\n\n\n1+1 \n\n2\n\n\n\n\n\nand figures are also included either in the main text:\n\nOr in the margin:\n\n\n\n\n\n\nThe sub-topics of statistics can be seen in the navigation bar at the top. For me the logical way to approach it is from left to right, but feel free to do it however you like."
  },
  {
    "objectID": "quarto_venv/lib/python3.10/site-packages/pyzmq-26.4.0.dist-info/licenses/LICENSE.html",
    "href": "quarto_venv/lib/python3.10/site-packages/pyzmq-26.4.0.dist-info/licenses/LICENSE.html",
    "title": "Statistics Notes",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2009-2012, Brian Granger, Min Ragan-Kelley\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "quarto_venv/lib/python3.10/site-packages/idna-3.10.dist-info/LICENSE.html",
    "href": "quarto_venv/lib/python3.10/site-packages/idna-3.10.dist-info/LICENSE.html",
    "title": "Statistics Notes",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2013-2024, Kim Davies and contributors. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "quarto_venv/lib/python3.10/site-packages/httpcore-1.0.9.dist-info/licenses/LICENSE.html",
    "href": "quarto_venv/lib/python3.10/site-packages/httpcore-1.0.9.dist-info/licenses/LICENSE.html",
    "title": "Statistics Notes",
    "section": "",
    "text": "Copyright © 2020, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "bayes.html",
    "href": "bayes.html",
    "title": "Introduction to bayesian thinking:",
    "section": "",
    "text": "Introduction to bayesian thinking:"
  }
]